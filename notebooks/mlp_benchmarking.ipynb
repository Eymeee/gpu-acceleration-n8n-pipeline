{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d487172",
   "metadata": {},
   "source": [
    "# Benchmarking of CPU vs GPU for a Multi Layer Perceptron on a Small Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43cecae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment if necessary\n",
    "# %pip install torch sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa1779",
   "metadata": {},
   "source": [
    "### create the Multi Layer Perceptron model  \n",
    "- 3 layers   \n",
    "- dynamic input layer  \n",
    "- 32 nodes in 1st hidden layer  \n",
    "- 16 nodes in 2nd hidden layer  \n",
    "- 2 classes for output layer   \n",
    "- ReLU for activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b6034fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module): \n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=16)\n",
    "        self.fc3 = nn.Linear(in_features=16, out_features=2)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x)) \n",
    "        logits = self.fc3(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f75b33a",
   "metadata": {},
   "source": [
    "### we will be using the `heart disease UCI` dataset for this benchmarking, I already prepared it, we will clone the following repo to get the `.joblib` ready data from it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eef027cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'deep-learning-labs' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/zakariaaithssain/deep-learning-labs.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "076cfb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/content/deep-learning-labs/data/ready/ready.joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc849a98",
   "metadata": {},
   "source": [
    "### get data from the `joblib` file   \n",
    "the joblib file contains the following `json`-like structure:   \n",
    "```python3 \n",
    "{\n",
    "    \"train\": {\n",
    "            \"X\": X_train, \n",
    "            \"y\": y_train\n",
    "            }, \n",
    "    \"test\": {\n",
    "            \"X\": X_test, \n",
    "            \"y\": y_test\n",
    "        }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e69c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "\n",
    "\n",
    "try:\n",
    "    data = joblib.load(DATA_PATH)\n",
    "except FileNotFoundError: \n",
    "        print(f\"{DATA_PATH} not found.\")\n",
    "        exit(1)\n",
    "\n",
    "training_data = data[\"train\"]\n",
    "X_train = training_data[\"X\"]\n",
    "y_train = training_data[\"y\"]\n",
    "\n",
    "#convert to tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32) #float is required for gradient calcs\n",
    "y_train = torch.tensor(y_train, dtype=torch.long) #long type is issential for cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f615d6",
   "metadata": {},
   "source": [
    "### define `train` function with benchmarking using `time.perf_counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e190a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, dataloader, num_epochs:int, lr:float, device): \n",
    "        #move model to device\n",
    "        model.to(device, non_blocking=True)\n",
    "        #Adam with L2 regularization, regularization strength 1e-4\n",
    "        decay, no_decay = [], []\n",
    "        for _, param in model.named_parameters():\n",
    "            if param.ndim == 1:  #bias\n",
    "                no_decay.append(param)\n",
    "            else: #weights\n",
    "                decay.append(param)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {\"params\": decay, \"weight_decay\": 1e-4},\n",
    "                {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "            ],\n",
    "            lr=lr\n",
    "        )\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for epoch in range(num_epochs): \n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                n_batches = 0\n",
    "                for X_batch, y_batch in dataloader: \n",
    "                        # to calculate the mean of losses per batch (train_loss/n_batches)\n",
    "                        n_batches+=1\n",
    "                        #tensors should be at the same device as the model\n",
    "                        X_batch = X_batch.to(device, non_blocking=True)\n",
    "                        y_batch = y_batch.to(device, non_blocking=True)\n",
    "                        #forward pass\n",
    "                        logits = model(X_batch)\n",
    "                        batch_loss = criterion(logits, y_batch)\n",
    "\n",
    "                        train_loss+= batch_loss.item()\n",
    "\n",
    "                        #backward pass\n",
    "                        optimizer.zero_grad()\n",
    "                        batch_loss.backward()\n",
    "\n",
    "                        #update params \n",
    "                        optimizer.step()\n",
    "                train_loss/= n_batches\n",
    "                \n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        end = time.perf_counter()\n",
    "        return end - start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb40c6",
   "metadata": {},
   "source": [
    "#### we use the following formula to calculate the graphical acceleration: \n",
    "**SPEED UP = CPU TIME / GPU TIME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ab49a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU training started.\n",
      "GPU time (s):  27.890532811999947\n",
      "CPU training started.\n",
      "CPU time (s):  27.748056419999784\n",
      "SPEED UP:  0.9948915858667691\n"
     ]
    }
   ],
   "source": [
    "num_features = X_train.size(dim=1)\n",
    "model = MultiLayerPerceptron(input_dim=num_features)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True,\n",
    "                                            pin_memory=True, num_workers=2) #for GPU training, not need for CPU\n",
    "\n",
    "num_epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    print(\"GPU training started.\")\n",
    "    gpu_time = train(model, dataloader, num_epochs, lr, torch.device(\"cuda\"))\n",
    "    print(\"GPU time (s): \", gpu_time)\n",
    "\n",
    "    print(\"CPU training started.\")\n",
    "    cpu_time = train(model, dataloader, num_epochs, lr, torch.device(\"cpu\"))\n",
    "    print(\"CPU time (s): \", cpu_time)\n",
    "    speed_up = cpu_time/gpu_time \n",
    "    print(\"SPEED UP: \", speed_up)\n",
    "else: \n",
    "    print(\"gpu not available for benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf4156",
   "metadata": {},
   "source": [
    "### CONCLUSION:  \n",
    "for this 3 layers MLP model with less than 900 training data points, combining: \n",
    " - GPU usage\n",
    " - parallel data preprocessing in CPU (num_workers > 0 in DataLoader)\n",
    " - memory pinning (pin_memory is True in DataLoader)\n",
    " - non blocking CPU scheduling (non_blocking is True when transfering to device)\n",
    "\n",
    " makes training **slower** than using CPU as the device **(SPEED UP < 1)**, especially if the batch size is small.   \n",
    " this is due to the transfer time from host to GPU being the bottleneck, and also the GPU requires warming up time before reaching its full potential. \n",
    "\n",
    " **Note:** GPU only gives acceleration for heavy tasks, the heaviest the better. using GPU for liteweight tasks (e.g small datasets, small models) causes latency instead of acceleration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
